import streamlit as st
import os
import time
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Asistente 4 Materias - Dual Mode",
    page_icon="üéì",
    layout="wide"
)

# ==================== IMPORTACIONES CONDICIONALES ====================
SENTENCE_TRANSFORMERS_AVAILABLE = False
TRANSFORMERS_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    st.warning("‚ö†Ô∏è sentence-transformers no disponible. Usando modo fallback.")

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    st.warning("‚ö†Ô∏è transformers no disponible. IA Generativa deshabilitada.")

# ==================== DEFINICIONES ====================
PROFESORES = {
    "estadistica": {
        "nombre": "Estad√≠stica",
        "emoji": "üìä",
        "profesor": "Prof. Matem√°ticas",
        "consejo": "Practica con ejercicios y memoriza las f√≥rmulas clave",
        "ejemplos_busqueda": [
            "Ejercicio 3 de la gu√≠a 2",
            "F√≥rmula de la media ponderada",
            "Fecha del parcial"
        ],
        "ejemplos_ia": [
            "Expl√≠came el teorema de Bayes",
            "¬øC√≥mo estudio para el parcial?",
            "Diferencia entre media y mediana"
        ]
    },
    "campo_laboral": {
        "nombre": "Campo Laboral",
        "emoji": "üíº",
        "profesor": "Prof. Acri",
        "consejo": "Participa activamente y prepara excelentes presentaciones",
        "ejemplos_busqueda": [
            "Requisitos del trabajo pr√°ctico",
            "Consejos para entrevistas",
            "Criterios de evaluaci√≥n"
        ],
        "ejemplos_ia": [
            "¬øC√≥mo preparo una buena entrevista?",
            "Qu√© valora la profesora Acri?",
            "Consejos para mi CV"
        ]
    },
    "algoritmos": {
        "nombre": "Algoritmos",
        "emoji": "üíª",
        "profesor": "Prof. C√≥digo",
        "consejo": "Codifica, practica y entiende la complejidad",
        "ejemplos_busqueda": [
            "Algoritmo de ordenamiento quicksort",
            "Complejidad de b√∫squeda binaria",
            "Ejercicio 5 de recursi√≥n"
        ],
        "ejemplos_ia": [
            "¬øCu√°ndo usar recursi√≥n vs iteraci√≥n?",
            "Expl√≠came la notaci√≥n Big O",
            "¬øC√≥mo optimizo mi c√≥digo?"
        ]
    },
    "redes": {
        "nombre": "Redes",
        "emoji": "üåê",
        "profesor": "Prof. Conectividad",
        "consejo": "Entiende los protocolos y practica con casos reales",
        "ejemplos_busqueda": [
            "Modelo OSI capas",
            "Protocolo TCP vs UDP",
            "Configuraci√≥n de subredes"
        ],
        "ejemplos_ia": [
            "¬øC√≥mo funciona el protocolo HTTP?",
            "Diferencias entre router y switch",
            "¬øQu√© es el DNS?"
        ]
    }
}

# ==================== UI ====================
st.title("üéì Asistente 4 Materias")
st.markdown("**B√∫squeda Sem√°ntica üîç + IA Generativa ü§ñ**")

# ==================== SIDEBAR ====================
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    materia_seleccionada = st.selectbox(
        "üìö Materia:",
        options=list(PROFESORES.keys()),
        format_func=lambda x: f"{PROFESORES[x]['emoji']} {PROFESORES[x]['nombre']}"
    )
    
    materia = PROFESORES[materia_seleccionada]
    
    st.markdown("---")
    
    # Selector de modo - solo si hay al menos un sistema disponible
    if SENTENCE_TRANSFORMERS_AVAILABLE or TRANSFORMERS_AVAILABLE:
        opciones_modo = []
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            opciones_modo.append("busqueda")
        if TRANSFORMERS_AVAILABLE:
            opciones_modo.append("ia_generativa")
        
        if len(opciones_modo) > 1:
            modo = st.radio(
                "üéØ Modo de Respuesta:",
                options=opciones_modo,
                format_func=lambda x: "üîç B√∫squeda Sem√°ntica" if x == "busqueda" else "ü§ñ IA Generativa"
            )
        else:
            modo = opciones_modo[0]
            st.info(f"Modo activo: {'üîç B√∫squeda' if modo == 'busqueda' else 'ü§ñ IA'}")
    else:
        st.error("‚ùå Ning√∫n sistema disponible")
        modo = "busqueda"
    
    st.markdown("---")
    st.markdown(f"**üë®‚Äçüè´ Profesor:** {materia['profesor']}")
    st.markdown(f"**üí° Consejo:** {materia['consejo']}")
    
    st.markdown("---")
    
    with st.expander("üîß Configuraci√≥n Avanzada"):
        cantidad_resultados = st.slider("Resultados:", 1, 7, 3)
        temperatura = st.slider("Temperatura IA:", 0.1, 1.0, 0.7, 0.1)
    
    st.markdown("---")
    
    if st.button("üßπ Limpiar Conversaci√≥n", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ==================== FUNCIONES AUXILIARES ====================
def dividir_texto(texto, tamano_chunk=800, overlap=150):
    """Dividir texto en chunks con overlap"""
    chunks = []
    inicio = 0
    
    while inicio < len(texto):
        fin = inicio + tamano_chunk
        chunk = texto[inicio:fin]
        
        if fin < len(texto):
            ultimo_punto = chunk.rfind('.')
            ultimo_salto = chunk.rfind('\n')
            corte = max(ultimo_punto, ultimo_salto)
            if corte > tamano_chunk * 0.6:
                chunk = chunk[:corte+1]
                fin = inicio + corte + 1
        
        if chunk.strip():
            chunks.append(chunk.strip())
        
        inicio = fin - overlap
        if inicio >= len(texto):
            break
    
    return chunks

def cargar_documentos():
    """Cargar todos los documentos de la carpeta conocimiento"""
    if not os.path.exists("conocimiento"):
        return [], []
    
    textos = []
    metadatos = []
    
    for materia_dir in os.listdir("conocimiento"):
        materia_path = os.path.join("conocimiento", materia_dir)
        if os.path.isdir(materia_path):
            for archivo in os.listdir(materia_path):
                if archivo.endswith('.txt'):
                    archivo_path = os.path.join(materia_path, archivo)
                    try:
                        with open(archivo_path, 'r', encoding='utf-8') as f:
                            contenido = f.read().strip()
                            if contenido:
                                chunks = dividir_texto(contenido, 800, 150)
                                for i, chunk in enumerate(chunks):
                                    textos.append(chunk)
                                    metadatos.append({
                                        "materia": materia_dir,
                                        "archivo": archivo,
                                        "fuente": f"{materia_dir}/{archivo}",
                                        "chunk": i
                                    })
                    except Exception:
                        continue
    
    return textos, metadatos

# ==================== SISTEMA DE B√öSQUEDA SEM√ÅNTICA ====================
@st.cache_resource
def inicializar_busqueda_semantica():
    """Inicializar sistema de b√∫squeda con sentence-transformers"""
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        return None, None, None
    
    try:
        modelo = SentenceTransformer('all-MiniLM-L6-v2')
        textos, metadatos = cargar_documentos()
        
        if not textos:
            st.warning("‚ö†Ô∏è No se encontraron documentos")
            return None, None, None
        
        with st.spinner("üîÑ Generando embeddings..."):
            embeddings = modelo.encode(textos, show_progress_bar=False, convert_to_numpy=True)
        
        st.success(f"‚úÖ B√∫squeda sem√°ntica lista: {len(textos)} fragmentos")
        return modelo, embeddings, (textos, metadatos)
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")
        return None, None, None

# ==================== SISTEMA FALLBACK TF-IDF ====================
@st.cache_resource
def inicializar_busqueda_tfidf():
    """Sistema de b√∫squeda fallback con TF-IDF"""
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        textos, metadatos = cargar_documentos()
        
        if not textos:
            st.warning("‚ö†Ô∏è No se encontraron documentos")
            return None, None, None
        
        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        matriz_tfidf = vectorizer.fit_transform(textos)
        
        st.success(f"‚úÖ B√∫squeda TF-IDF lista: {len(textos)} fragmentos")
        return vectorizer, matriz_tfidf, (textos, metadatos)
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")
        return None, None, None

# ==================== SISTEMA IA GENERATIVA ====================
@st.cache_resource
def inicializar_ia_generativa():
    """Inicializar modelo generativo"""
    if not TRANSFORMERS_AVAILABLE:
        return None, None
    
    try:
        model_name = "distilgpt2"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        modelo = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        st.success("‚úÖ IA Generativa lista")
        return modelo, tokenizer
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")
        return None, None

# ==================== INICIALIZACI√ìN ====================
with st.spinner("üîÑ Inicializando sistemas..."):
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        modelo_embed, embeddings_bd, datos = inicializar_busqueda_semantica()
    else:
        modelo_embed, embeddings_bd, datos = inicializar_busqueda_tfidf()
    
    if TRANSFORMERS_AVAILABLE:
        modelo_gen, tokenizer_gen = inicializar_ia_generativa()
    else:
        modelo_gen, tokenizer_gen = None, None

# ==================== FUNCIONES DE B√öSQUEDA ====================
def buscar_con_embeddings(consulta, modelo, embeddings, datos, materia_obj, k):
    """Buscar usando embeddings de sentence-transformers"""
    textos, metadatos = datos
    query_embedding = modelo.encode([consulta], convert_to_numpy=True)
    similitudes = cosine_similarity(query_embedding, embeddings)[0]
    indices_ordenados = np.argsort(similitudes)[::-1]
    
    resultados = []
    for idx in indices_ordenados:
        if similitudes[idx] < 0.1:
            continue
        if materia_obj and metadatos[idx]["materia"] != materia_obj:
            continue
        
        resultados.append({
            "texto": textos[idx],
            "metadata": metadatos[idx],
            "score": float(similitudes[idx])
        })
        
        if len(resultados) >= k:
            break
    
    return resultados

def buscar_con_tfidf(consulta, vectorizer, matriz, datos, materia_obj, k):
    """Buscar usando TF-IDF"""
    textos, metadatos = datos
    query_vector = vectorizer.transform([consulta])
    similitudes = cosine_similarity(query_vector, matriz)[0]
    indices_ordenados = np.argsort(similitudes)[::-1]
    
    resultados = []
    for idx in indices_ordenados:
        if similitudes[idx] < 0.05:
            continue
        if materia_obj and metadatos[idx]["materia"] != materia_obj:
            continue
        
        resultados.append({
            "texto": textos[idx],
            "metadata": metadatos[idx],
            "score": float(similitudes[idx])
        })
        
        if len(resultados) >= k:
            break
    
    return resultados

def buscar_informacion(consulta, materia_obj, k):
    """Buscar informaci√≥n relevante"""
    if datos is None:
        return [], "Sistema no disponible"
    
    try:
        if SENTENCE_TRANSFORMERS_AVAILABLE and modelo_embed is not None:
            resultados = buscar_con_embeddings(consulta, modelo_embed, embeddings_bd, datos, materia_obj, k)
        else:
            resultados = buscar_con_tfidf(consulta, modelo_embed, embeddings_bd, datos, materia_obj, k)
        
        if not resultados:
            return [], "No encontr√© informaci√≥n relevante"
        
        return resultados, None
    except Exception as e:
        return [], f"Error: {str(e)}"

def generar_respuesta_busqueda(consulta, resultados):
    """Generar respuesta formateada para b√∫squeda"""
    if not resultados:
        return "‚ùå No encontr√© informaci√≥n espec√≠fica.\n\nüí° Intenta reformular tu pregunta."
    
    respuesta = f"**üîç Resultados para: \"{consulta}\"**\n\n"
    
    por_archivo = {}
    for r in resultados:
        fuente = r['metadata']['fuente']
        if fuente not in por_archivo:
            por_archivo[fuente] = []
        por_archivo[fuente].append(r)
    
    for fuente, items in por_archivo.items():
        respuesta += f"**üìÅ {fuente}**\n\n"
        for item in items:
            texto = item['texto']
            if len(texto) > 600:
                texto = texto[:600] + "..."
            
            score = item['score']
            relevancia = "üü¢" if score > 0.4 else "üü°" if score > 0.25 else "üü†"
            respuesta += f"{relevancia} {texto}\n\n"
    
    respuesta += f"---\n**üí° {len(resultados)} fragmentos encontrados**"
    return respuesta

def generar_respuesta_ia(consulta, resultados, temp):
    """Generar respuesta con IA"""
    if modelo_gen is None:
        return "‚ùå IA Generativa no disponible"
    
    contexto = "\n".join([r['texto'][:300] for r in resultados[:2]]) if resultados else "Material general"
    
    prompt = f"Pregunta: {consulta}\nContexto: {contexto}\nRespuesta:"
    
    try:
        inputs = tokenizer_gen(prompt, return_tensors="pt", max_length=400, truncation=True)
        
        with torch.no_grad():
            outputs = modelo_gen.generate(
                inputs.input_ids,
                max_new_tokens=150,
                temperature=temp,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer_gen.eos_token_id
            )
        
        respuesta = tokenizer_gen.decode(outputs[0], skip_special_tokens=True)
        respuesta = respuesta[len(prompt):].strip()
        
        if len(respuesta) > 500:
            respuesta = respuesta[:500] + "..."
        
        return f"{respuesta}\n\n---\n**ü§ñ Generado por IA**"
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

# ==================== CHAT ====================
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": f"¬°Hola! üëã Soy tu asistente para **{materia['nombre']}** {materia['emoji']}"
    }]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input(f"Pregunta sobre {materia['nombre']}..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("üîç Procesando..."):
            resultados, error = buscar_informacion(prompt, materia_seleccionada, cantidad_resultados)
            
            if modo == "busqueda":
                respuesta = generar_respuesta_busqueda(prompt, resultados) if not error else f"‚ùå {error}"
            else:
                respuesta = generar_respuesta_ia(prompt, resultados, temperatura)
        
        placeholder = st.empty()
        texto = ""
        for linea in respuesta.split('\n'):
            texto += linea + '\n'
            time.sleep(0.02)
            placeholder.markdown(texto + "‚ñå")
        placeholder.markdown(texto)
    
    st.session_state.messages.append({"role": "assistant", "content": texto})

# ==================== FOOTER ====================
st.markdown("---")
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric("Modo", "üîç" if modo == "busqueda" else "ü§ñ")
with col2:
    st.metric("Materia", materia['emoji'])
with col3:
    st.metric("B√∫squeda", "üü¢" if datos else "üî¥")
with col4:
    st.metric("IA", "üü¢" if modelo_gen else "üî¥")

st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #28a745; font-weight: bold;'>"
    "üéì ASISTENTE 4 MATERIAS - DUAL MODE AI"
    "</div>",
    unsafe_allow_html=True
)
