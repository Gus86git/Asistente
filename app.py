import streamlit as st
import time
import os

# Configuraci√≥n
st.set_page_config(
    page_title="Asistente 4 Materias - Dos Modos",
    page_icon="üéì",
    layout="wide"
)

# T√≠tulo
st.title("üéì Asistente 4 Materias - Elige tu Modo de Estudio")
st.markdown("### B√∫squeda Sem√°ntica ü§ù IA Generativa - Lo mejor de ambos mundos")

# Configuraci√≥n de materias y profesores
PROFESORES = {
    "estadistica": {
        "nombre": "Estad√≠stica",
        "emoji": "üìä",
        "profesor": "Prof. Ferrarre",
        "consejo": "Practica TODOS los ejercicios y enf√≥cate en entender el proceso paso a paso",
        "color": "blue"
    },
    "desarrollo_ia": {
        "nombre": "Desarrollo de IA", 
        "emoji": "ü§ñ",
        "profesor": "Especialista IA",
        "consejo": "Entiende los fundamentos antes de usar frameworks complejos",
        "color": "green"
    },
    "campo_laboral": {
        "nombre": "Campo Laboral",
        "emoji": "üíº",
        "profesor": "Prof. Acri",
        "consejo": "S√© impecable en presentaciones y prepara exhaustivamente cada entrega",
        "color": "orange"
    },
    "comunicacion": {
        "nombre": "Comunicaci√≥n",
        "emoji": "üéØ", 
        "profesor": "Especialista Comunicaci√≥n",
        "consejo": "Estructura tus mensajes y adapta tu lenguaje al p√∫blico",
        "color": "purple"
    }
}

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    # Selector de materia
    materia_seleccionada = st.selectbox(
        "üìö Selecciona la materia:",
        list(PROFESORES.keys()),
        format_func=lambda x: f"{PROFESORES[x]['emoji']} {PROFESORES[x]['nombre']}"
    )
    
    materia = PROFESORES[materia_seleccionada]
    
    # Selector de MODO
    st.markdown("---")
    st.header("üéõÔ∏è Modo de Estudio")
    
    modo = st.radio(
        "Elige c√≥mo quieres estudiar:",
        ["busqueda", "ia_generativa"],
        format_func=lambda x: {
            "busqueda": "üîç B√∫squeda Sem√°ntica",
            "ia_generativa": "ü§ñ IA Generativa"
        }[x],
        help="Selecciona el modo que mejor se adapte a tu necesidad"
    )
    
    # Explicaci√≥n de cada modo
    with st.expander("üìñ ¬øQu√© significa cada modo?", expanded=True):
        if modo == "busqueda":
            st.success("**üîç B√öSQUEDA SEM√ÅNTICA**")
            st.markdown("""
            **‚úÖ Ventajas:**
            - Encuentra informaci√≥n EXACTA de tus archivos
            - Muestra las fuentes (sabes de d√≥nde viene)
            - Muy r√°pido y confiable
            - Ideal para buscar informaci√≥n espec√≠fica
            
            **‚ö†Ô∏è Limitaciones:**
            - Solo recupera informaci√≥n existente
            - No explica ni resume autom√°ticamente
            """)
        else:
            st.info("**ü§ñ IA GENERATIVA**")
            st.markdown("""
            **‚úÖ Ventajas:**
            - Explica conceptos de manera natural
            - Responde preguntas complejas
            - Sintetiza informaci√≥n de m√∫ltiples fuentes
            - Suena como un tutor real
            
            **‚ö†Ô∏è Limitaciones:**
            - Puede ocasionalmente inventar informaci√≥n
            - Un poco m√°s lento
            - Requiere m√°s recursos
            """)
    
    # Mostrar informaci√≥n del profesor
    st.markdown("---")
    st.markdown(f"### {materia['emoji']} {materia['profesor']}")
    st.markdown(f"**Consejo clave:** {materia['consejo']}")
    
    st.markdown("---")
    
    if st.button("üßπ Limpiar Conversaci√≥n", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# Sistema de B√∫squeda Sem√°ntica
@st.cache_resource
def inicializar_busqueda_semantica():
    """Inicializar el sistema de b√∫squeda sem√°ntica"""
    try:
        from langchain.embeddings import HuggingFaceEmbeddings
        from langchain.vectorstores import FAISS
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain.schema import Document
        
        # Verificar que existe la carpeta conocimiento
        if not os.path.exists("conocimiento"):
            st.error("‚ùå No se encuentra la carpeta 'conocimiento'")
            return None
        
        # Cargar embeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # Cargar todos los documentos
        documentos = []
        archivos_cargados = 0
        
        for materia_dir in os.listdir("conocimiento"):
            materia_path = os.path.join("conocimiento", materia_dir)
            if os.path.isdir(materia_path):
                for archivo in os.listdir(materia_path):
                    if archivo.endswith('.txt'):
                        archivo_path = os.path.join(materia_path, archivo)
                        try:
                            with open(archivo_path, 'r', encoding='utf-8') as f:
                                contenido = f.read().strip()
                                if contenido:
                                    documentos.append(
                                        Document(
                                            page_content=contenido,
                                            metadata={
                                                "materia": materia_dir,
                                                "archivo": archivo,
                                                "fuente": f"{materia_dir}/{archivo}"
                                            }
                                        )
                                    )
                                    archivos_cargados += 1
                        except Exception as e:
                            continue
        
        if not documentos:
            st.error("‚ùå No se encontraron documentos con contenido")
            return None
        
        # Dividir documentos en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        documentos_divididos = text_splitter.split_documents(documentos)
        
        # Crear base de datos vectorial
        vectorstore = FAISS.from_documents(documentos_divididos, embeddings)
        
        st.success(f"‚úÖ B√∫squeda sem√°ntica lista: {archivos_cargados} archivos")
        return vectorstore
        
    except Exception as e:
        st.error(f"‚ùå Error en b√∫squeda sem√°ntica: {str(e)}")
        return None

# Sistema de IA Generativa
@st.cache_resource
def inicializar_ia_generativa():
    """Inicializar el modelo de IA generativa"""
    try:
        from transformers import pipeline
        import torch
        
        # Usar un modelo m√°s ligero para Streamlit Cloud
        model = pipeline(
            "text-generation",
            model="microsoft/DialoGPT-medium",  # Modelo liviano y r√°pido
            torch_dtype=torch.float16,
            max_length=1024
        )
        st.success("‚úÖ IA Generativa lista")
        return model
    except Exception as e:
        st.error(f"‚ùå Error en IA generativa: {str(e)}")
        return None

# Inicializar sistemas
with st.spinner("üîÑ Inicializando sistemas..."):
    buscador = inicializar_busqueda_semantica()
    modelo_ia = inicializar_ia_generativa()

# Funci√≥n de b√∫squeda sem√°ntica
def buscar_informacion_relevante(consulta, materia_objetivo=None, cantidad_resultados=3):
    """Buscar informaci√≥n relevante en todo el material"""
    if buscador is None:
        return [], "Sistema de b√∫squeda no disponible"
    
    try:
        documentos_encontrados = buscador.similarity_search(consulta, k=cantidad_resultados)
        
        if materia_objetivo:
            docs_filtrados = [doc for doc in documentos_encontrados 
                            if doc.metadata.get("materia") == materia_objetivo]
            if docs_filtrados:
                documentos_encontrados = docs_filtrados
        
        if not documentos_encontrados:
            return [], "No encontr√© informaci√≥n espec√≠fica para tu b√∫squeda."
        
        return documentos_encontrados, None
        
    except Exception as e:
        return [], f"Error en la b√∫squeda: {str(e)}"

# Funci√≥n para respuesta de B√öSQUEDA SEM√ÅNTICA
def generar_respuesta_busqueda(consulta, documentos_encontrados, materia):
    """Generar respuesta para modo b√∫squeda sem√°ntica"""
    
    if not documentos_encontrados:
        return "No encontr√© informaci√≥n espec√≠fica para tu b√∫squeda."
    
    respuesta = f"**üîç B√∫squeda Sem√°ntica - Resultados para: \"{consulta}\"**\n\n"
    
    # Agrupar por archivo
    archivos_vistos = set()
    contenido_por_archivo = {}
    
    for doc in documentos_encontrados:
        archivo = doc.metadata.get("fuente", "Desconocido")
        if archivo not in contenido_por_archivo:
            contenido_por_archivo[archivo] = []
        contenido_por_archivo[archivo].append(doc.page_content)
    
    # Mostrar contenido de cada archivo
    for archivo, contenidos in contenido_por_archivo.items():
        respuesta += f"**üìÅ {archivo}**\n\n"
        for i, contenido in enumerate(contenidos, 1):
            if len(contenido) > 500:
                contenido = contenido[:500] + "..."
            respuesta += f"{contenido}\n\n"
    
    respuesta += "---\n"
    respuesta += "**üí° Modo B√∫squeda Sem√°ntica:** Est√°s viendo informaci√≥n EXACTA de tus archivos. Ideal para encontrar datos espec√≠ficos."
    
    return respuesta

# Funci√≥n para respuesta de IA GENERATIVA
def generar_respuesta_ia(consulta, documentos_encontrados, materia, modelo):
    """Generar respuesta usando IA generativa"""
    
    if modelo is None:
        return "El modo IA Generativa no est√° disponible en este momento."
    
    if not documentos_encontrados:
        contexto = f"Informaci√≥n general sobre {PROFESORES[materia]['nombre']}"
    else:
        # Combinar toda la informaci√≥n encontrada
        contexto = "\n\n".join([doc.page_content for doc in documentos_encontrados])
    
    try:
        # Prompt para el modelo
        prompt = f"""
        Eres un tutor educativo especializado en {PROFESORES[materia]['nombre']}.
        Est√°s ayudando a un estudiante universitario.

        INFORMACI√ìN DE CONTEXTO (de los materiales del curso):
        {contexto}

        PREGUNTA DEL ESTUDIANTE:
        {consulta}

        Proporciona una respuesta educativa, clara y √∫til bas√°ndote en la informaci√≥n anterior.
        Si la informaci√≥n no es suficiente, s√© honesto y sugiere d√≥nde podr√≠a encontrar m√°s informaci√≥n.
        Responde en espa√±ol de manera natural y conversacional.

        RESPUESTA:
        """
        
        # Generar respuesta
        respuesta = modelo(
            prompt,
            max_new_tokens=400,
            temperature=0.7,
            do_sample=True,
            pad_token_id=model.tokenizer.eos_token_id
        )
        
        generated_text = respuesta[0]['generated_text']
        
        # Extraer solo la parte de la respuesta
        if "RESPUESTA:" in generated_text:
            respuesta_texto = generated_text.split("RESPUESTA:")[-1].strip()
        else:
            respuesta_texto = generated_text
        
        # Agregar informaci√≥n sobre las fuentes si hay documentos
        if documentos_encontrados:
            archivos = set(doc.metadata.get("fuente") for doc in documentos_encontrados)
            respuesta_texto += f"\n\n---\n**üìö Fuentes consultadas:** {', '.join(archivos)}"
        
        respuesta_texto += "\n\n**ü§ñ Modo IA Generativa:** Esta respuesta fue generada por IA bas√°ndose en tu material. Puede contener interpretaciones."
        
        return respuesta_texto
        
    except Exception as e:
        return f"Error generando respuesta: {str(e)}"

# Inicializar chat
if "messages" not in st.session_state:
    mensaje_inicial = f"¬°Hola! Soy tu asistente para {PROFESORES[materia_seleccionada]['nombre']}. "
    
    if modo == "busqueda":
        mensaje_inicial += "Estoy en **modo B√∫squeda Sem√°ntica** - encontrar√© informaci√≥n exacta de tus archivos. üîç"
    else:
        mensaje_inicial += "Estoy en **modo IA Generativa** - explicar√© conceptos de manera natural. ü§ñ"
    
    st.session_state.messages = [
        {"role": "assistant", "content": mensaje_inicial}
    ]

# Mostrar historial de chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input del usuario - DIFERENTE seg√∫n el modo
placeholder_text = {
    "busqueda": f"Buscar en {PROFESORES[materia_seleccionada]['nombre']}...",
    "ia_generativa": f"Preguntar sobre {PROFESORES[materia_seleccionada]['nombre']}..."
}

if prompt := st.chat_input(placeholder_text[modo]):
    # Agregar mensaje del usuario
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Procesar seg√∫n el modo seleccionado
    with st.chat_message("assistant"):
        if modo == "busqueda":
            with st.spinner("üîç Buscando en tus archivos..."):
                documentos_encontrados, error = buscar_informacion_relevante(
                    prompt, 
                    materia_seleccionada,
                    cantidad_resultados=3
                )
                
                if error:
                    respuesta = f"**{error}**\n\n"
                    respuesta += "üí° **Sugerencia:** Intenta con otras palabras clave o cambia al modo IA Generativa."
                else:
                    respuesta = generar_respuesta_busqueda(prompt, documentos_encontrados, materia_seleccionada)
        
        else:  # Modo IA Generativa
            with st.spinner("ü§ñ Analizando y generando respuesta..."):
                # Primero buscar informaci√≥n relevante
                documentos_encontrados, _ = buscar_informacion_relevante(
                    prompt, 
                    materia_seleccionada,
                    cantidad_resultados=3
                )
                
                # Luego generar respuesta con IA
                respuesta = generar_respuesta_ia(prompt, documentos_encontrados, materia_seleccionada, modelo_ia)
        
        # Efecto de escritura
        placeholder = st.empty()
        respuesta_completa = ""
        
        for chunk in respuesta.split('\n'):
            respuesta_completa += chunk + '\n'
            time.sleep(0.03)
            placeholder.markdown(respuesta_completa + "‚ñå")
        
        placeholder.markdown(respuesta_completa)
    
    st.session_state.messages.append({"role": "assistant", "content": respuesta_completa})

# Panel de informaci√≥n
st.markdown("---")
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric(
        label="Modo Actual", 
        value="üîç B√∫squeda" if modo == "busqueda" else "ü§ñ IA"
    )

with col2:
    st.metric(
        label="Materia", 
        value=PROFESORES[materia_seleccionada]['emoji']
    )

with col3:
    status_busqueda = "üü¢" if buscador else "üî¥"
    st.metric("B√∫squeda", status_busqueda)

with col4:
    status_ia = "üü¢" if modelo_ia else "üî¥"
    st.metric("IA Generativa", status_ia)

# Gu√≠a de uso
st.markdown("---")
st.success("""
**üéØ Gu√≠a de Uso - Cu√°ndo usar cada modo:**

### üîç **B√öSQUEDA SEM√ÅNTICA - Usa cuando:**
- Necesitas informaci√≥n EXACTA de tus archivos
- Quieres saber en qu√© archivo est√° la informaci√≥n
- Buscas datos espec√≠ficos (fechas, ejercicios, conceptos puntuales)
- Prefieres velocidad y confiabilidad absoluta

### ü§ñ **IA GENERATIVA - Usa cuando:**
- Quieres explicaciones naturales y conversacionales
- Necesitas que sinteticen informaci√≥n de m√∫ltiples fuentes
- Tienes preguntas complejas que requieren razonamiento
- Prefieres respuestas m√°s elaboradas y contextuales

### üí° **Consejo:** ¬°Prueba ambos modos y ve cu√°l te funciona mejor para cada situaci√≥n!
""")

# Ejemplos espec√≠ficos por modo y materia
st.markdown("---")
col_ej1, col_ej2 = st.columns(2)

with col_ej1:
    st.markdown("**üîç Ejemplos para B√∫squeda Sem√°ntica:**")
    if materia_seleccionada == "estadistica":
        st.markdown("- 'Ejercicio 3 de la gu√≠a 2'")
        st.markdown("- 'F√≥rmula de la media ponderada'")
        st.markdown("- 'Fecha del parcial'")
    elif materia_seleccionada == "campo_laboral":
        st.markdown("- 'Requisitos del trabajo pr√°ctico'")
        st.markdown("- 'Consejos para entrevistas'")
        st.markdown("- 'Evaluaci√≥n de la presentaci√≥n'")

with col_ej2:
    st.markdown("**ü§ñ Ejemplos para IA Generativa:**")
    if materia_seleccionada == "estadistica":
        st.markdown("- 'Expl√≠came el teorema de Bayes'")
        st.markdown("- '¬øC√≥mo estudio para el parcial?'")
        st.markdown("- 'Diferencia entre media y mediana'")
    elif materia_seleccionada == "campo_laboral":
        st.markdown("- '¬øC√≥mo preparo una buena entrevista?'")
        st.markdown("- 'Qu√© valora m√°s la profesora Acri?'")
        st.markdown("- 'Consejos para mi CV'")

st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: green; font-weight: bold;'>"
    "üéì ASISTENTE 4 MATERIAS - DOS MODOS, INFINITAS POSIBILIDADES DE APRENDIZAJE"
    "</div>",
    unsafe_allow_html=True
)

